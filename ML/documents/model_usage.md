# Piggy ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ

## ë¹ ë¥¸ ì‹œì‘

### ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš© (ê¶Œì¥)
1. Flask ì•± ì‹¤í–‰: `python app.py`
2. ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:5000` ì ‘ì†
3. ì¬ë¬´ ì •ë³´ ì…ë ¥ í›„ "AI ì¬ë¬´ê±´ì „ì„± ë¶„ì„ ì‹œì‘" í´ë¦­

### API ì—”ë“œí¬ì¸íŠ¸

#### POST /predict
ì¬ë¬´ ë°ì´í„°ë¥¼ ë°›ì•„ ê±´ì „ì„± ì ìˆ˜ì™€ ìœ„í—˜ë„ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

**ìš”ì²­ í˜•ì‹:**
```json
{
    "total_spending": 250.0,
    "mean_spending": 1.5,
    "n_transactions": 150,
    "education": 0.10,
    "transport": 0.15,
    "other": 0.20,
    "medical": 0.12,
    "food": 0.18,
    "entertainment": 0.10,
    "housing": 0.15,
    "income": 350.0
}
```

**ì‘ë‹µ í˜•ì‹:**
```json
{
    "score": 65.2,
    "risk_label": 1,
    "risk_info": {
        "level": "ì£¼ì˜",
        "color": "yellow",
        "description": "ì•½ê°„ì˜ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤"
    },
    "persona": {
        "name": "ê· í˜•ìˆ˜ë‹¬",
        "emoji": "koala",
        "description": "ì ì ˆí•œ ì†Œë¹„ì™€ ì €ì¶•ì˜ ê· í˜•",
        "level": 4
    },
    "probabilities": [0.1, 0.6, 0.25, 0.05],
    "savings": 100.0,
    "savings_rate": 28.6,
    "income": 350.0,
    "total_spending": 250.0
}
```

## ì…ë ¥ ë°ì´í„° ê°€ì´ë“œ

### í•„ìˆ˜ ì…ë ¥ (11ê°œ)

| í•„ë“œ | íƒ€ì… | ë²”ìœ„ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|------|------|
| `total_spending` | float | 50-2000 | ì›” ì´ ì§€ì¶œì•¡ (ë§Œì›) | 250.0 |
| `mean_spending` | float | 0.1-50 | í‰ê·  ê±°ë˜ ê¸ˆì•¡ (ë§Œì›) | 1.5 |
| `n_transactions` | int | 10-1000 | ì›” ê±°ë˜ ê±´ìˆ˜ | 150 |
| `income` | float | 100-3000 | ì›” ì†Œë“ (ë§Œì›) | 350.0 |
| `education` | float | 0.0-1.0 | êµìœ¡ìœ¡ì•„ ì§€ì¶œ ë¹„ìœ¨ | 0.10 |
| `transport` | float | 0.0-1.0 | êµí†µë¹„ ì§€ì¶œ ë¹„ìœ¨ | 0.15 |
| `other` | float | 0.0-1.0 | ê¸°íƒ€ì†Œë¹„ ì§€ì¶œ ë¹„ìœ¨ | 0.20 |
| `medical` | float | 0.0-1.0 | ë³´ê±´ì˜ë£Œ ì§€ì¶œ ë¹„ìœ¨ | 0.12 |
| `food` | float | 0.0-1.0 | ì‹ë£Œí’ˆìŒë£Œ ì§€ì¶œ ë¹„ìœ¨ | 0.18 |
| `entertainment` | float | 0.0-1.0 | ì˜¤ë½ë¬¸í™” ì§€ì¶œ ë¹„ìœ¨ | 0.10 |
| `housing` | float | 0.0-1.0 | ì£¼ê±°ë¹„ ì§€ì¶œ ë¹„ìœ¨ | 0.15 |

### ë°ì´í„° ê²€ì¦ ê·œì¹™

1. **ë¹„ìœ¨ í•©ê³„**: ì§€ì¶œ ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨ì˜ í•©ì´ 0.9-1.1 ë²”ìœ„ì—¬ì•¼ í•¨
2. **ì–‘ìˆ˜ ì¡°ê±´**: ëª¨ë“  ê¸ˆì•¡ê³¼ ê±´ìˆ˜ëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•¨
3. **ë…¼ë¦¬ì  ì¼ê´€ì„±**: `mean_spending â‰ˆ total_spending / n_transactions`

## ì¶œë ¥ í•´ì„ ê°€ì´ë“œ

### ì ìˆ˜ êµ¬ê°„ë³„ ì˜ë¯¸

| ì ìˆ˜ | ë“±ê¸‰ | í˜ë¥´ì†Œë‚˜ ì˜ˆì‹œ | íŠ¹ì§• |
|------|------|---------------|------|
| 80-100 | ë§¤ìš° ìš°ìˆ˜ | ğŸ¦ì €ì¶•ì™•, ğŸ¯ê¸ˆìœµê³ ìˆ˜ | ì €ì¶•ë¥  20%+, ì¬ëŸ‰ì§€ì¶œ 25%â†“ |
| 60-79 | ì–‘í˜¸ | ğŸ¨ì €ì¶•ë‹¬ì¸, ğŸ¼ê· í˜•ì¡íŒ | ì €ì¶•ë¥  10-20%, ì•ˆì •ì  ê´€ë¦¬ |
| 40-59 | ë³´í†µ | ğŸ°ì ˆì•½ëŸ¬, ğŸ±ë³´í†µ | ì €ì¶•ë¥  0-10%, ê°œì„  ì—¬ì§€ |
| 0-39 | ê°œì„ í•„ìš” | ğŸ¹ì¬ë¬´ì´ˆë³´, ğŸ¦Šì ììœ„í—˜ | ì ì ë˜ëŠ” ì €ì¶•ë¥  0%â†“ |

### ìœ„í—˜ë„ ë“±ê¸‰

- **0 (ì•ˆì „)**: ì¬ë¬´ìƒíƒœ ë§¤ìš° ê±´ì „, ì§€ì† ìœ ì§€
- **1 (ì£¼ì˜)**: ì•½ê°„ì˜ ê°œì„  í•„ìš”, ì˜ˆë°©ì  ì¡°ì¹˜
- **2 (ìœ„í—˜)**: ì ê·¹ì  ê°œì„  í•„ìš”, ì§€ì¶œ êµ¬ì¡° ì¡°ì •
- **3 (ê³ ìœ„í—˜)**: ê¸´ê¸‰ ê°œì„  í•„ìš”, ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥

## ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

### JavaScript (í”„ë¡ íŠ¸ì—”ë“œ)
```javascript
async function analyzeFinance(userData) {
    try {
        const response = await fetch('/predict', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(userData)
        });
        
        const result = await response.json();
        
        // ê²°ê³¼ í™œìš©
        displayScore(result.score);
        showPersona(result.persona);
        updateRiskIndicator(result.risk_info);
        
        return result;
    } catch (error) {
        console.error('ë¶„ì„ ì‹¤íŒ¨:', error);
    }
}

// ì‚¬ìš© ì˜ˆì‹œ
const userData = {
    total_spending: 280,
    mean_spending: 1.8,
    n_transactions: 156,
    education: 0.08,
    transport: 0.15,
    other: 0.10,
    medical: 0.12,
    food: 0.18,
    entertainment: 0.12,
    housing: 0.25,
    income: 350
};

analyzeFinance(userData);
```

### Python (ë°±ì—”ë“œ/ë¶„ì„)
```python
import requests
import json

def get_financial_analysis(data):
    """ì¬ë¬´ ë¶„ì„ API í˜¸ì¶œ"""
    url = 'http://localhost:5000/predict'
    headers = {'Content-Type': 'application/json'}
    
    response = requests.post(url, headers=headers, json=data)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"API ì˜¤ë¥˜: {response.status_code}")

# ì‚¬ìš© ì˜ˆì‹œ
user_data = {
    "total_spending": 320,
    "mean_spending": 2.5,
    "n_transactions": 128,
    "education": 0.03,
    "transport": 0.18,
    "other": 0.04,
    "medical": 0.05,
    "food": 0.20,
    "entertainment": 0.15,
    "housing": 0.35,
    "income": 280
}

result = get_financial_analysis(user_data)
print(f"ì ìˆ˜: {result['score']}")
print(f"í˜ë¥´ì†Œë‚˜: {result['persona']['name']}")
print(f"ì €ì¶•ë¥ : {result['savings_rate']}%")
```

## ëª¨ë¸ ì§ì ‘ ì‚¬ìš© (ê³ ê¸‰)

### ëª¨ë¸ íŒŒì¼ ë¡œë“œ
```python
import pickle
import pandas as pd
import numpy as np

# ëª¨ë¸ ë¡œë“œ
with open('piggy_model_v2.pkl', 'rb') as f:
    model_data = pickle.load(f)

print(f"ëª¨ë¸ íŠ¹ì„±: {model_data['feature_names']}")
print(f"í•™ìŠµ ë©”íƒ€ë°ì´í„°: {model_data['training_metadata']}")
```

### íŒŒìƒ íŠ¹ì„± ìƒì„± í•¨ìˆ˜
```python
def create_derived_features(df):
    """íŒŒìƒ íŠ¹ì„± ìë™ ìƒì„±"""
    df = df.copy()
    
    # ì €ì¶•ë¥ 
    df['savings_rate'] = (df['est_income_ë§Œì›'] - df['total_spending']) / df['est_income_ë§Œì›']
    df['savings_rate'] = df['savings_rate'].clip(-1, 1)
    
    # ì§€ì¶œì†Œë“ë¹„ìœ¨
    df['spending_income_ratio'] = df['total_spending'] / df['est_income_ë§Œì›']
    
    # í•„ìˆ˜ì§€ì¶œë¹„ìœ¨
    df['essential_spending_ratio'] = df['ì£¼ê±°'] + df['ì‹ë£Œí’ˆìŒë£Œ'] + df['ë³´ê±´ì˜ë£Œ']
    
    # ì¬ëŸ‰ì§€ì¶œë¹„ìœ¨
    df['discretionary_spending_ratio'] = df['ì˜¤ë½ë¬¸í™”'] + df['ê¸°íƒ€ì†Œë¹„']
    
    # í‰ê· ê±°ë˜í¬ê¸°
    df['avg_transaction_size'] = df['total_spending'] / (df['n_transactions'] + 1)
    
    # ì†Œë“êµ¬ê°„ ë”ë¯¸ë³€ìˆ˜
    df['income_low'] = (df['est_income_ë§Œì›'] < 300).astype(int)
    df['income_mid'] = ((df['est_income_ë§Œì›'] >= 300) & (df['est_income_ë§Œì›'] < 600)).astype(int)
    df['income_high'] = (df['est_income_ë§Œì›'] >= 600).astype(int)
    
    return df

# ì‚¬ìš© ì˜ˆì‹œ
input_df = pd.DataFrame([{
    'total_spending': 250,
    'mean_spending': 1.5,
    'n_transactions': 150,
    'êµìœ¡ìœ¡ì•„': 0.10,
    'êµí†µ': 0.15,
    'ê¸°íƒ€ì†Œë¹„': 0.20,
    'ë³´ê±´ì˜ë£Œ': 0.12,
    'ì‹ë£Œí’ˆìŒë£Œ': 0.18,
    'ì˜¤ë½ë¬¸í™”': 0.10,
    'ì£¼ê±°': 0.15,
    'est_income_ë§Œì›': 350
}])

# íŒŒìƒ íŠ¹ì„± ìƒì„±
input_df = create_derived_features(input_df)

# ì˜ˆì¸¡
X = input_df[model_data['feature_names']].values
X_scaled = model_data['scaler'].transform(X)

score = model_data['regressor'].predict(X_scaled)[0]
risk_class = model_data['classifier'].predict(X_scaled)[0]
probabilities = model_data['classifier'].predict_proba(X_scaled)[0]

print(f"ì˜ˆì¸¡ ì ìˆ˜: {score:.1f}")
print(f"ìœ„í—˜ ë“±ê¸‰: {risk_class}")
print(f"ë“±ê¸‰ë³„ í™•ë¥ : {probabilities}")
```

## ë°°ì¹˜ ì²˜ë¦¬

### ì—¬ëŸ¬ ì‚¬ìš©ì ë™ì‹œ ë¶„ì„
```python
def batch_analyze(users_data):
    """ì—¬ëŸ¬ ì‚¬ìš©ì ë°ì´í„° ë°°ì¹˜ ë¶„ì„"""
    results = []
    
    for user_id, data in users_data.items():
        try:
            result = get_financial_analysis(data)
            result['user_id'] = user_id
            results.append(result)
        except Exception as e:
            print(f"ì‚¬ìš©ì {user_id} ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
users = {
    'user_001': {
        'total_spending': 250, 'income': 350,
        'mean_spending': 1.5, 'n_transactions': 150,
        # ... ë‚˜ë¨¸ì§€ í•„ë“œ
    },
    'user_002': {
        'total_spending': 400, 'income': 500,
        'mean_spending': 2.2, 'n_transactions': 180,
        # ... ë‚˜ë¨¸ì§€ í•„ë“œ
    }
}

batch_results = batch_analyze(users)
```

## ì˜¤ë¥˜ ì²˜ë¦¬

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ë°©ë²•

1. **ë¹„ìœ¨ í•©ê³„ ì˜¤ë¥˜**
   ```
   ì˜¤ë¥˜: ì§€ì¶œ ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨ì˜ í•©ì´ 1.0Â±0.1 ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨
   í•´ê²°: ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨ì˜ í•©ì´ 0.9-1.1ì´ ë˜ë„ë¡ ì¡°ì •
   ```

2. **ìŒìˆ˜ê°’ ì˜¤ë¥˜**
   ```
   ì˜¤ë¥˜: ê¸ˆì•¡ì´ë‚˜ ê±´ìˆ˜ì— ìŒìˆ˜ê°’ í¬í•¨
   í•´ê²°: ëª¨ë“  ê¸ˆì•¡ê³¼ ê±´ìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œ ì…ë ¥
   ```

3. **ë²”ìœ„ ì´ˆê³¼ ì˜¤ë¥˜**
   ```
   ì˜¤ë¥˜: ì…ë ¥ê°’ì´ í—ˆìš© ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨
   í•´ê²°: ê° í•„ë“œì˜ í—ˆìš© ë²”ìœ„ í™•ì¸ í›„ ì¬ì…ë ¥
   ```

### ì—ëŸ¬ í•¸ë“¤ë§ ì˜ˆì‹œ
```python
def safe_analyze(data):
    """ì•ˆì „í•œ ë¶„ì„ í•¨ìˆ˜ (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)"""
    try:
        # ë°ì´í„° ê²€ì¦
        validate_input_data(data)
        
        # ë¶„ì„ ìˆ˜í–‰
        result = get_financial_analysis(data)
        
        return {
            'success': True,
            'data': result
        }
        
    except ValueError as e:
        return {
            'success': False,
            'error': f'ì…ë ¥ ë°ì´í„° ì˜¤ë¥˜: {str(e)}'
        }
    except requests.RequestException as e:
        return {
            'success': False,
            'error': f'API í†µì‹  ì˜¤ë¥˜: {str(e)}'
        }
    except Exception as e:
        return {
            'success': False,
            'error': f'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {str(e)}'
        }

def validate_input_data(data):
    """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
    required_fields = [
        'total_spending', 'mean_spending', 'n_transactions', 'income',
        'education', 'transport', 'other', 'medical', 
        'food', 'entertainment', 'housing'
    ]
    
    # í•„ìˆ˜ í•„ë“œ í™•ì¸
    for field in required_fields:
        if field not in data:
            raise ValueError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
    
    # ì–‘ìˆ˜ í™•ì¸
    positive_fields = ['total_spending', 'mean_spending', 'n_transactions', 'income']
    for field in positive_fields:
        if data[field] <= 0:
            raise ValueError(f"{field}ëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    # ë¹„ìœ¨ í•©ê³„ í™•ì¸
    ratio_fields = ['education', 'transport', 'other', 'medical', 'food', 'entertainment', 'housing']
    ratio_sum = sum(data[field] for field in ratio_fields)
    if not (0.9 <= ratio_sum <= 1.1):
        raise ValueError(f"ì§€ì¶œ ë¹„ìœ¨ í•©ê³„ê°€ {ratio_sum:.3f}ì…ë‹ˆë‹¤. 0.9-1.1 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤")
```

## ì„±ëŠ¥ ìµœì í™”

### ìºì‹± í™œìš©
```python
from functools import lru_cache
import hashlib
import json

@lru_cache(maxsize=1000)
def cached_analyze(data_hash):
    """ê²°ê³¼ ìºì‹±ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”"""
    # ì‹¤ì œ ë¶„ì„ ë¡œì§
    pass

def get_data_hash(data):
    """ë°ì´í„° í•´ì‹œ ìƒì„±"""
    data_str = json.dumps(data, sort_keys=True)
    return hashlib.md5(data_str.encode()).hexdigest()

# ì‚¬ìš© ì˜ˆì‹œ
data_hash = get_data_hash(user_data)
result = cached_analyze(data_hash)
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### ì‚¬ìš©ëŸ‰ ì¶”ì 
```python
import logging
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('piggy_usage.log'),
        logging.StreamHandler()
    ]
)

def log_prediction(user_data, result, processing_time):
    """ì˜ˆì¸¡ ê²°ê³¼ ë¡œê¹…"""
    log_data = {
        'timestamp': datetime.now().isoformat(),
        'input_hash': get_data_hash(user_data),
        'score': result.get('score'),
        'risk_level': result.get('risk_label'),
        'processing_time_ms': processing_time * 1000
    }
    
    logging.info(f"Prediction: {json.dumps(log_data)}")
```

ì´ ê°€ì´ë“œë¥¼ í†µí•´ Piggy ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ì§€ì›ì´ í•„ìš”í•œ ê²½ìš° ê°œë°œíŒ€ì— ë¬¸ì˜í•˜ì„¸ìš”.
